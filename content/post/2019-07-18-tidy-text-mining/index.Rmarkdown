---
title: Tidy Text Mining
author: Yuan Du
date: '2019-07-19'
slug: tidy-text-mining
categories:
  - R
tags:
  - Tidytext
  - Tidyverse
subtitle: ''
summary: ''
authors: []
lastmod: '2019-08-19T00:18:01-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
Thanks to the short course at SDSS 2019, I learned how to do tf-idf to topic modeling and sentiment analysis by using tidytext taught by Julia Silge, author of [Text Mining with R](https://www.tidytextmining.com/) and Mara Averick. They did a great job on teaching the four hour class. I didn't expect to have so much covered in the short course. 

Here is an example that I used the method to analyze **A Tale of Two Cities** and **Great Expectations** by Charles Dickens by using sentiment analysis.

1. **Install the `tidytext` package for text mining.**

`install.packages("tidytext")`

2. **Read the book from gutenbergr package, after install gutenbergr package**

We can Downloading books by ID (98 and 1400) from [Project Gutenbergr](http://www.gutenberg.org/).

```{r read, echo=TRUE, message=FALSE}

library(gutenbergr)
library(dplyr)

book <-  gutenberg_download(c(98, 1400), meta_fields = "title") %>%
  group_by(title) %>%
  mutate(line = row_number()) %>%
  ungroup()

book

```
3. **Process books into chapters and words in tidy data form**

we need to restructure it as one-token-per-row format. As pre-processing, we divide these into chapters, use tidytext’s `unnest_tokens` to separate them into words, then remove `stop_word`s. We’re treating every chapter as a separate “document”, each with a name like *A Tale of Two cities* or *Great Expectations*.
```{r unnest, include=TRUE, message=FALSE}
library(tidytext)
tidy_book <- book %>%
  unnest_tokens(word, text)

tidy_book


tidy_book <- tidy_book %>%
  anti_join(get_stopwords())

#We can also use count to find the most common words in all the book as a whole
tidy_book %>%
  count(word, sort = TRUE)
```

4. **Sentiment analysis**

Sentiment analysis can be done as an inner join. Three sentiment lexicons are available via the `get_sentiments()` function. Let's examine how sentiment changes during each novel. Let's find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.

```{r, message=FALSE}
library(tidyr)
get_sentiments("bing")

sentiment <- tidy_book %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(title, index = line %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

sentiment
```

**Now we can plot these sentiment scores across the plot trajectory of each novel.**
```{r, message=FALSE}
library(ggplot2)

ggplot(sentiment, aes(index, sentiment, fill = title)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free_x")
```

It looks like that **A Table of Two Cities** has more negative emotions and **Great Expectations** is more balanced on emotions.

5. **Most common positive and negative words**

One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment.

```{r, message=FALSE}
bing_word_counts <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

bing_word_counts
```

This can be shown visually, and we can pipe straight into ggplot2 because of the way we are consistently using tools built for handling tidy data frames.
```{r, message=FALSE}
bing_word_counts %>%
  filter(n > 100) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```
This lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom `stop-words` list using `bind_rows`.

```{r, message=FALSE}
custom_stop_words <- bind_rows(get_stopwords(),
                               tibble(word = "miss",
                                          lexicon = "custom"))

tidy_book2 <- tidy_book %>%
  anti_join(custom_stop_words) %>%
  count(word, sort = TRUE)

```
6. **Wordclouds**

We’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well.

For example, consider the wordcloud package. Let’s look at the most common words in Charles Dickens’ two books as a whole again.

```{r, message=FALSE，warning=FALSE}
library(wordcloud)

tidy_book %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

In other functions, such as comparison.cloud, you may need to turn it into a matrix with `reshape2`’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format.
```{r, message=FALSE, warning=FALSE}
library(reshape2)

tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

*For "Converting to and from Document-Term Matrix and Corpus objects", You can visit [here](https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html).*
